{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a01f77e9-8e5a-451d-9ffb-e50d1441e787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Variables seleccionadas: ['Xtwice', 'Xstop', 'Xdet', 'Xprep', 'Xmentions']\n",
      "Filas antes del dropna: 961\n",
      "Filas después del dropna: 960\n",
      "\n",
      "📦 Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 1 → loss=0.6942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 2 → loss=0.6904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 3 → loss=0.6876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 4 → loss=0.6759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 5 → loss=0.6236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 6 → loss=0.4902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 7 → loss=0.2770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 8 → loss=0.1218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 9 → loss=0.0589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3276556/4012079607.py:175: RuntimeWarning: coroutine 'Translator.translate' was never awaited\n",
      "  txt_es = \"\"\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 10 → loss=0.0346\n",
      "✅ Fold 1 F1=0.5613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    femenino       0.49      0.60      0.54        82\n",
      "   masculino       0.64      0.54      0.58       110\n",
      "\n",
      "    accuracy                           0.56       192\n",
      "   macro avg       0.57      0.57      0.56       192\n",
      "weighted avg       0.58      0.56      0.56       192\n",
      "\n",
      "\n",
      "📦 Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 1 → loss=0.6926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 2 → loss=0.6886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 3 → loss=0.6847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 4 → loss=0.6661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 5 → loss=0.5775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 6 → loss=0.3825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 7 → loss=0.1833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 8 → loss=0.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 9 → loss=0.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3276556/4012079607.py:175: RuntimeWarning: coroutine 'Translator.translate' was never awaited\n",
      "  txt_es = \"\"\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 10 → loss=0.0278\n",
      "✅ Fold 2 F1=0.5720\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    femenino       0.51      0.53      0.52        83\n",
      "   masculino       0.63      0.61      0.62       109\n",
      "\n",
      "    accuracy                           0.58       192\n",
      "   macro avg       0.57      0.57      0.57       192\n",
      "weighted avg       0.58      0.58      0.58       192\n",
      "\n",
      "\n",
      "📦 Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 1 → loss=0.6957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 2 → loss=0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 3 → loss=0.6883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 4 → loss=0.6836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 5 → loss=0.6578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 6 → loss=0.5656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 7 → loss=0.3956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 8 → loss=0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 9 → loss=0.1287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3276556/4012079607.py:175: RuntimeWarning: coroutine 'Translator.translate' was never awaited\n",
      "  txt_es = \"\"\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 10 → loss=0.0671\n",
      "✅ Fold 3 F1=0.5843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    femenino       0.55      0.45      0.49        83\n",
      "   masculino       0.63      0.72      0.68       109\n",
      "\n",
      "    accuracy                           0.60       192\n",
      "   macro avg       0.59      0.59      0.58       192\n",
      "weighted avg       0.60      0.60      0.60       192\n",
      "\n",
      "\n",
      "📦 Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 1 → loss=0.6954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 2 → loss=0.6938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 3 → loss=0.6806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 4 → loss=0.6440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 5 → loss=0.5349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 6 → loss=0.3474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 7 → loss=0.2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 8 → loss=0.1094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 9 → loss=0.0567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3276556/4012079607.py:175: RuntimeWarning: coroutine 'Translator.translate' was never awaited\n",
      "  txt_es = \"\"\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 10 → loss=0.0489\n",
      "✅ Fold 4 F1=0.5304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    femenino       0.47      0.57      0.51        83\n",
      "   masculino       0.60      0.50      0.55       109\n",
      "\n",
      "    accuracy                           0.53       192\n",
      "   macro avg       0.53      0.54      0.53       192\n",
      "weighted avg       0.54      0.53      0.53       192\n",
      "\n",
      "\n",
      "📦 Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 1 → loss=0.6946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 2 → loss=0.6905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 3 → loss=0.6749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 4 → loss=0.6409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 5 → loss=0.5254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 6 → loss=0.3349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 7 → loss=0.1659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 8 → loss=0.0988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 9 → loss=0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3276556/4012079607.py:175: RuntimeWarning: coroutine 'Translator.translate' was never awaited\n",
      "  txt_es = \"\"\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 10 → loss=0.0399\n",
      "✅ Fold 5 F1=0.5343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    femenino       0.47      0.54      0.50        83\n",
      "   masculino       0.60      0.53      0.57       109\n",
      "\n",
      "    accuracy                           0.54       192\n",
      "   macro avg       0.54      0.54      0.53       192\n",
      "weighted avg       0.55      0.54      0.54       192\n",
      "\n",
      "\n",
      "📊 F1 por fold → [0.5613, 0.572, 0.5843, 0.5304, 0.5343]\n",
      "🏁 F1 macro medio: 0.5565\n",
      "📝 Tweets claros → outputs_gender/clear_tweets_gender.json\n",
      "🔒 Mejor modelo guardado (fold 3, F1=0.5843)\n",
      "💾 Scaler guardado en scaler_gender.pkl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fine-tuning multimodal RoBERTa + tabular para GENDER.\n",
    "Guarda checkpoints por fold, el mejor modelo, y un JSON\n",
    "con los tuits que clasifica correctamente con confianza ≥ CONF_TH,\n",
    "incluyendo la traducción automática al español.\n",
    "\"\"\"\n",
    "\n",
    "# ─────────────────── IMPORTS ───────────────────\n",
    "import pathlib, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from googletrans import Translator        # pip install googletrans==4.0.0-rc1\n",
    "from joblib import dump\n",
    "\n",
    "tr = Translator()\n",
    "\n",
    "# ─────────────────── CONFIG ────────────────────\n",
    "MODEL_NAME   = \"cardiffnlp/twitter-roberta-base\"\n",
    "INPUT_CSV    = \"features_linguisticas_en_con_glove.csv\"\n",
    "\n",
    "BATCH_SIZE   = 16\n",
    "EPOCHS       = 10\n",
    "LR           = 1e-5\n",
    "FOLDS        = 5\n",
    "CONF_TH      = 0.70           # confianza mínima para “tweets claros”\n",
    "\n",
    "ckpt_dir = pathlib.Path(\"checkpoints_gender\"); ckpt_dir.mkdir(exist_ok=True)\n",
    "out_dir  = pathlib.Path(\"outputs_gender\");     out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ──────────────── CARGA Y PRE-PROCESADO ─────────\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# → sólo femenino / masculino\n",
    "df = df[df[\"gender\"].isin([\"Female\", \"Male\"])].copy()\n",
    "df[\"label\"] = df[\"gender\"].map({\"Female\": 0, \"Male\": 1})\n",
    "\n",
    "\n",
    "# Variables tabulares elegidas (basadas en Cohen’s d y viabilidad)\n",
    "selected_vars = ['Xtwice', 'Xstop', 'Xdet', 'Xprep', 'Xmentions']\n",
    "print(\"🔎 Variables seleccionadas:\", selected_vars)\n",
    "\n",
    "text_col = \"clean_text\"\n",
    "req_cols = [text_col, \"label\"] + selected_vars\n",
    "missing  = [c for c in req_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"❌ Faltan columnas en el CSV: {missing}\")\n",
    "\n",
    "# descarta filas con NaN en cualquiera de las columnas requeridas\n",
    "print(\"Filas antes del dropna:\", len(df))\n",
    "df = df[req_cols].dropna().reset_index(drop=True)\n",
    "print(\"Filas después del dropna:\", len(df))\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(\"❌ El DataFrame quedó vacío tras el filtrado; \"\n",
    "                     \"revisa las columnas y los NaN.\")\n",
    "\n",
    "# Escalado de las variables numéricas\n",
    "scaler = StandardScaler()\n",
    "df[selected_vars] = scaler.fit_transform(df[selected_vars])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ───────────────────── DATASET ──────────────────\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, texts, nums, labels):\n",
    "        self.texts, self.nums, self.labels = texts, nums, labels\n",
    "\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True, padding=\"max_length\", max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"nums\"]   = torch.tensor(self.nums[idx], dtype=torch.float32)\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# ───────────────────── MODELO ───────────────────\n",
    "class TransformerWithTabular(nn.Module):\n",
    "    def __init__(self, backbone, n_tab):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(backbone)\n",
    "        self.tabular_net = nn.Sequential(\n",
    "            nn.Linear(n_tab, 64), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.classifier  = nn.Sequential(\n",
    "            nn.Linear(768 + 32, 64), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, nums):\n",
    "        cls = self.transformer(input_ids, attention_mask).last_hidden_state[:, 0]\n",
    "        tab = self.tabular_net(nums)\n",
    "        return self.classifier(torch.cat([cls, tab], dim=1))\n",
    "\n",
    "# ────────────────── TRAIN / EVAL ────────────────\n",
    "texts    = df[text_col].tolist()\n",
    "features = df[selected_vars].to_numpy(dtype=np.float32)\n",
    "labels   = df[\"label\"].to_numpy()\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weights   = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\n",
    "loss_fn   = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32).to(device))\n",
    "skf       = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "f1_scores, selected = [], []\n",
    "best_f1, best_state, best_fold = 0.0, None, None\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(texts, labels), 1):\n",
    "    print(f\"\\n📦 Fold {fold}/{FOLDS}\")\n",
    "    train_ds = MultiModalDataset([texts[i] for i in tr_idx], features[tr_idx], labels[tr_idx])\n",
    "    val_ds   = MultiModalDataset([texts[i] for i in va_idx], features[va_idx], labels[va_idx])\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = TransformerWithTabular(MODEL_NAME, len(selected_vars)).to(device)\n",
    "    opt   = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # —— entrenamiento —— #\n",
    "    model.train()\n",
    "    for ep in range(1, EPOCHS + 1):\n",
    "        tot_loss = 0\n",
    "        for batch in tqdm(train_dl, desc=f\"Fold {fold}·Ep {ep}\", leave=False):\n",
    "            ids   = batch[\"input_ids\"].to(device)\n",
    "            attn  = batch[\"attention_mask\"].to(device)\n",
    "            nums  = batch[\"nums\"].to(device)\n",
    "            lbls  = batch[\"labels\"].to(device)\n",
    "\n",
    "            loss = loss_fn(model(ids, attn, nums), lbls)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tot_loss += loss.item()\n",
    "        print(f\"   Ep {ep} → loss={tot_loss/len(train_dl):.4f}\")\n",
    "\n",
    "    # —— validación —— #\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for b_idx, batch in enumerate(val_dl):\n",
    "            ids  = batch[\"input_ids\"].to(device)\n",
    "            attn = batch[\"attention_mask\"].to(device)\n",
    "            nums = batch[\"nums\"].to(device)\n",
    "            lbls = batch[\"labels\"].to(device)\n",
    "\n",
    "            probs = torch.softmax(model(ids, attn, nums), dim=1)\n",
    "            confs, preds = probs.max(dim=1)\n",
    "\n",
    "            y_true.extend(lbls.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "            # tuits claramente clasificados\n",
    "            mask = (preds == lbls) & (confs >= CONF_TH)\n",
    "            for loc in torch.where(mask)[0]:\n",
    "                idx = va_idx[b_idx * BATCH_SIZE + loc.item()]\n",
    "                txt = texts[idx]\n",
    "                try:\n",
    "                    txt_es = tr.translate(txt, dest=\"es\").text\n",
    "                except Exception:\n",
    "                    txt_es = \"\"\n",
    "                selected.append({\n",
    "                    \"row\": int(idx), \"text_en\": txt, \"text_es\": txt_es,\n",
    "                    \"prob\": round(confs[loc].item(), 4),\n",
    "                    \"pred\": int(preds[loc]), \"true\": int(lbls[loc]),\n",
    "                    \"fold\": fold\n",
    "                })\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"✅ Fold {fold} F1={f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"femenino\", \"masculino\"]))\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_fold = f1, fold\n",
    "        best_state = {\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"backbone\": MODEL_NAME,\n",
    "            \"selected_vars\": selected_vars\n",
    "        }\n",
    "\n",
    "    # checkpoint de la rama transformer\n",
    "    ckpt_fold = ckpt_dir / f\"fold{fold}\"\n",
    "    ckpt_fold.mkdir(exist_ok=True)\n",
    "    model.transformer.save_pretrained(ckpt_fold); tokenizer.save_pretrained(ckpt_fold)\n",
    "\n",
    "# ────────────────── RESULTADOS GLOBALES ─────────\n",
    "print(\"\\n📊 F1 por fold →\", [round(x,4) for x in f1_scores])\n",
    "print(f\"🏁 F1 macro medio: {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "# —— guardar tweets claros —— #\n",
    "json_path = out_dir / \"clear_tweets_gender.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(selected, fh, ensure_ascii=False, indent=2)\n",
    "print(f\"📝 Tweets claros → {json_path}\")\n",
    "\n",
    "# —— persistir mejor modelo y scaler —— #\n",
    "if best_state:\n",
    "    torch.save(best_state, out_dir / \"best_model_gender.pt\")\n",
    "    print(f\"🔒 Mejor modelo guardado (fold {best_fold}, F1={best_f1:.4f})\")\n",
    "dump(scaler, out_dir / \"scaler_gender.pkl\")\n",
    "print(\"💾 Scaler guardado en scaler_gender.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a15c2d6e-41c7-40e6-9d36-a8211a744313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 F1 macro por fold: [0.5613, 0.572, 0.5843, 0.5304, 0.5343]\n",
      "🏁 F1 macro promedio final: 0.5565\n",
      "📝 Guardados 492 tweets claros en /home/jupyter-lquijano/marcos/ingleses/outputs/clear_tweets_gender.json\n",
      "🔒 Guardado modelo completo en best_model.pt (fold 3, F1=0.5843)\n",
      "💾 Guardado StandardScaler en scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────── RESULTADOS FINALES ─────────\n",
    "print(\"\\n📊 F1 macro por fold:\", [round(f, 4) for f in f1_scores])\n",
    "print(f\"🏁 F1 macro promedio final: {np.mean(f1_scores):.4f}\")\n",
    "out_dir    = pathlib.Path(\"outputs\")\n",
    "# ----------- JSON con tweets claros --------------\n",
    "json_path = out_dir / \"clear_tweets_gender.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(selected, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📝 Guardados {len(selected)} tweets claros en {json_path.resolve()}\")\n",
    "\n",
    "# ----------- Guardar mejor modelo ---------------\n",
    "# Guardamos el mejor fold completo: arquitectura + pesos + vars\n",
    "torch.save(best_state, out_dir / \"best_model.pt\")\n",
    "print(f\"🔒 Guardado modelo completo en best_model.pt (fold {best_fold}, F1={best_f1:.4f})\")\n",
    "\n",
    "# ----------- Guardar scaler ---------------------\n",
    "from joblib import dump\n",
    "dump(scaler, out_dir / \"scaler.pkl\")\n",
    "print(\"💾 Guardado StandardScaler en scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f71e62-5b13-4c0b-b7e4-79edd5948541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Columnas reales:\n",
      "['XWE-GloVe_1', 'XWE-GloVe_10', 'XWE-GloVe_11', 'XWE-GloVe_12', 'XWE-GloVe_13', 'XWE-GloVe_14', 'XWE-GloVe_15', 'XWE-GloVe_16', 'XWE-GloVe_17', 'XWE-GloVe_18', 'XWE-GloVe_19', 'XWE-GloVe_2', 'XWE-GloVe_20', 'XWE-GloVe_21', 'XWE-GloVe_22', 'XWE-GloVe_23', 'XWE-GloVe_24', 'XWE-GloVe_25', 'XWE-GloVe_26', 'XWE-GloVe_27', 'XWE-GloVe_28', 'XWE-GloVe_29', 'XWE-GloVe_3', 'XWE-GloVe_30', 'XWE-GloVe_31', 'XWE-GloVe_32', 'XWE-GloVe_33', 'XWE-GloVe_34', 'XWE-GloVe_35', 'XWE-GloVe_36', 'XWE-GloVe_37', 'XWE-GloVe_38', 'XWE-GloVe_39', 'XWE-GloVe_4', 'XWE-GloVe_40', 'XWE-GloVe_41', 'XWE-GloVe_42', 'XWE-GloVe_43', 'XWE-GloVe_44', 'XWE-GloVe_45', 'XWE-GloVe_46', 'XWE-GloVe_47', 'XWE-GloVe_48', 'XWE-GloVe_49', 'XWE-GloVe_5', 'XWE-GloVe_50', 'XWE-GloVe_51', 'XWE-GloVe_52', 'XWE-GloVe_53', 'XWE-GloVe_54', 'XWE-GloVe_55', 'XWE-GloVe_56', 'XWE-GloVe_57', 'XWE-GloVe_58', 'XWE-GloVe_59', 'XWE-GloVe_6', 'XWE-GloVe_60', 'XWE-GloVe_61', 'XWE-GloVe_62', 'XWE-GloVe_63', 'XWE-GloVe_64', 'XWE-GloVe_65', 'XWE-GloVe_66', 'XWE-GloVe_67', 'XWE-GloVe_68', 'XWE-GloVe_69', 'XWE-GloVe_7', 'XWE-GloVe_70', 'XWE-GloVe_71', 'XWE-GloVe_72', 'XWE-GloVe_73', 'XWE-GloVe_74', 'XWE-GloVe_75', 'XWE-GloVe_76', 'XWE-GloVe_77', 'XWE-GloVe_78', 'XWE-GloVe_79', 'XWE-GloVe_8', 'XWE-GloVe_80', 'XWE-GloVe_81', 'XWE-GloVe_82', 'XWE-GloVe_83', 'XWE-GloVe_84', 'XWE-GloVe_85', 'XWE-GloVe_86', 'XWE-GloVe_87', 'XWE-GloVe_88', 'XWE-GloVe_89', 'XWE-GloVe_9', 'XWE-GloVe_90', 'Xadv', 'Xav_length', 'Xcapital', 'Xchar_par', 'Xcharacter', 'Xconj', 'Xdet', 'Xemoji', 'Xfuture', 'Xhashtag', 'Xlength_3', 'Xlength_6', 'Xmax_length', 'Xmentions', 'Xnouns', 'Xnumbers', 'Xpast', 'Xplural', 'Xprep', 'Xpronouns', 'Xpunctuation', 'Xsentence', 'Xsentence_par', 'Xsing', 'Xstop', 'Xtwice', 'Xunique', 'Xurl', 'Xword_par', 'Xwords', 'age', 'clean_text', 'country', 'ethnicity', 'gender', 'ideology', 'name', 'party', 'raw_text', 'source', 'tweet_id']\n",
      "\n",
      "👥 Distribución de gender:\n",
      "gender\n",
      "Male      547\n",
      "Female    414\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Filas tras filtrar género: 961\n",
      "\n",
      "📊 Porcentaje de NaN por columna:\n",
      "Xtwice       0.0\n",
      "Xstop        0.0\n",
      "Xdet         0.0\n",
      "Xprep        0.0\n",
      "Xmentions    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ──────── CARGA DE DATOS ────────\n",
    "df_raw = pd.read_csv(\"features_linguisticas_en_con_glove.csv\")\n",
    "\n",
    "# a) columnas reales\n",
    "print(\"\\n🔍 Columnas reales:\")\n",
    "print(sorted(df_raw.columns.tolist()))\n",
    "\n",
    "# b) distribución de género (sin filtrar)\n",
    "print(\"\\n👥 Distribución de gender:\")\n",
    "print(df_raw[\"gender\"].value_counts(dropna=False))\n",
    "\n",
    "# c) filtrar a solo \"Male\" y \"Female\"\n",
    "df_g = df_raw[df_raw[\"gender\"].isin([\"Male\", \"Female\"])].copy()\n",
    "print(\"\\nFilas tras filtrar género:\", len(df_g))\n",
    "\n",
    "# d) % de NaN por cada variable candidata\n",
    "vars_try = ['Xtwice', 'Xstop', 'Xdet', 'Xprep', 'Xmentions']\n",
    "print(\"\\n📊 Porcentaje de NaN por columna:\")\n",
    "print(df_g[vars_try].isna().mean().round(3) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7696f-aeae-442a-bb1c-4e373565ee7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
